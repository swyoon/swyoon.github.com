{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Stein Methods\n",
    "\n",
    "1. Stein's method\n",
    "2. Kernelized Stein Discrepancy\n",
    "3. Stein Gradient Estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stein's Method\n",
    "\n",
    "* Charles Stein 1972.\n",
    "* Originally developed to prove central limit theorem. \n",
    "    * I.e., to bound the difference between the sum of random variables and the \n",
    "* The method is generalized to obtain bounds on the distance between two probability distributions wrt probability metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Preliminary - Statistical Distance\n",
    "\n",
    "**Statistical Distance** is the measure of the difference between two probability distributions. The most well known example is Kullback-Leibler divergence, and I assume many of readers are already familiar with it. Each statistical distance quantify the difference from different perspectives, and also possesses different computational properties. Studying the differences and properties of statistical distances is a fundamental topic in machine learning research. Note that many of statistical distances are not actually a valid _distance_.\n",
    "\n",
    "Here, we consider a specific form of statistical difference, which is defined with a supremum and a **test function** $h$.\n",
    "\n",
    "$$\n",
    "d(P,Q) = \\sup_{h \\in \\mathcal{H}} \\left|\\int h dP - \\int h dQ\\right| = \\sup_{h \\in \\mathcal{H}} \\left| E [h(W)] - E [h(Y)] \\right|,\n",
    "$$\n",
    "where $P$ and $Q$ are probability measures, $W$, and $Y$ are random variables with distributions $P$ and $Q$.\n",
    "\n",
    "A simple example of such statistical distance is **Kolmogorov-Smirnoff Statistic**, which is widely used for hypothesis test called Kolmogorov-Smirnoff (KS) test. The statistic measures the largest absolute deviation between two cumulative distributions, one of which is typically an empirical CDF and the other is the true CDF.\n",
    "\n",
    "![ks_img](ks.png)(Image source: Wikipedia)\n",
    "\n",
    "$$\n",
    "F_n(x) = \\frac{1}{N}\\sum_{i=1}^{N} I_{[-\\infty, x]}(X_i)\n",
    "$$\n",
    "$$\n",
    "D_n = \\sup_{x} |F_n(x) - F(x)|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Stein's Equation\n",
    "\n",
    "### Convergence in distribution\n",
    "\n",
    "A sequence of real-valued random variables $X_n$ converges to $X$ in distribution if\n",
    "$$\n",
    "\\lim_{n \\to \\infty}F_n(x)=F(x) \\text{, and equivalently,}  \\lim_{n \\to \\infty} P(X_n \\leq x) = P(X \\leq x)\n",
    "$$\n",
    "for all x.\n",
    "\n",
    "A similar, but a broader way to represent the convergence is that for all bounded continuous functions $h:R \\to R$,  \n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\infty} E [g(X_n)] = E[g(X)].\n",
    "$$\n",
    "In fact, it is not necessary to consider all possible $g$, but only a small class of $g$.\n",
    "\n",
    "**Example : ** $g(x) \\in \\{ I_{[-\\infty, t]}(x) | t \\in R \\}$ \n",
    "\n",
    "**Another Example : ** $g(x) \\in \\{ e^{itx} | t \\in R, i=\\sqrt{-1} \\}$\n",
    "\n",
    "However, if LHS and RHS are not equal, we can conclude that $X_n$ and $X$ do not have the same distribution. Stein was interested in bounding the deviation between the two terms.\n",
    "\n",
    "### Stein's Operator\n",
    "\n",
    "Stein's initial observation is that when a random variable $Z$ which follows the standard normal distribution $\\mathcal{N}(0,1)$, the following equation holds\n",
    "$$\n",
    "\\mathbb{E}_{Z}[f'(Z)-Zf(Z)]=0\n",
    "$$\n",
    "for all absolutely sontinuous $f$ with bounded derivatives. The proof is simply given by an integration by parts.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{Z}[f'(Z)]= \\int_{-\\infty}^{\\infty} p(z) f'(z) dz &= \\left[p(z)f(z)\\right]^{\\infty}_{\\infty} + \\int_{-\\infty}^{\\infty} z p(z) f(z) dz \\\\\n",
    "                                      &= \\int_{-\\infty}^{\\infty} z p(z) f(z) dz = \\mathbb{E}[Zf(Z)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "We can say the operations in LHS \"characterizes\" the standard normal distribution, because we can check if $Z$ follows the standard normal distribution by computing the LHS. People somtimes abbreviate the LHS by introducing **Stein's operator** $A$ that transforms the function $f$.\n",
    "$$\n",
    "(Af)(x)=f'(x)-xf(x)\n",
    "$$\n",
    "and therefore, the original equation becomes $\\mathbb{E}_{Z}[(Af)(Z)]=0$.\n",
    "\n",
    "\n",
    "### Stein Equation\n",
    "\n",
    "If a random variable $X$ is not generated from the standard normal distribution, but from an approximation of it, the RHS would not be zero. To quantify the deviation, we consider the following differential quation which relates the stein operator to the statistical distance. Given any bounded measureable function $h$, we can find (a set of) $f_{h}$ satisfying the following equation.\n",
    "\n",
    "$$\n",
    "f_{h}'(x)  - xf_h(x) = h(x) - E[h(Z)]\n",
    "$$\n",
    "When we take expectation of both sides,\n",
    "$$\n",
    "E[f_h'(X)  - xf_h(X)] = E[h(X)] - E[h(Z)]\n",
    "$$\n",
    "The RHS is a form of statistical distance, and the LHS is the form in the Stein operator. Remember that when $h$ is given, and $f_h$ follows.\n",
    "\n",
    "Since $Z$ is the standard normal, we can write the explicit form of $f$.\n",
    "$$\n",
    "f_h(x) = e^{x^2/2} \\int_{-\\infty}^{x} [h(t)-\\mathbb{E}h(Z)]e^{-t^2/2} dt\n",
    "$$\n",
    "However, we are not very much interested in the standard normal case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Stein Operator for the General Case\n",
    "\n",
    "Carefully observing the Stein's operator, we can see that $-Z$ is actually $p'(Z)/p(Z)$. Then, we can write the Stein operator for an arbitrary distribution with the density $p(Z)$.\n",
    "$$\n",
    "\\mathbb{E}_{Z}\\left[f'(Z)- \\frac{p'(Z)}{p(Z)}f(Z)\\right]=\\mathbb{E}_{Z}\\left[f'(Z)+ (\\log p(Z))'f(Z)\\right] = 0\n",
    "$$\n",
    "\n",
    "The proof is similar.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{Z}[f'(Z)]= \\int_{-\\infty}^{\\infty} p(z) f'(z) dz &= \\left[p(z)f(z)\\right]^{\\infty}_{\\infty} - \\int_{-\\infty}^{\\infty}  p'(z) f(z) dz \\\\\n",
    "                                      &= -\\int_{-\\infty}^{\\infty} \\frac{p'(z)}{p(z)}p(z) f(z) dz = -\\mathbb{E}[(\\log p(Z) )'f(Z)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Stein operator $A_p f$ is defined as $(A_p f)(x)= f'(x) + (\\log p(x))' f(x)$\n",
    "\n",
    "$\\mathbb{E}[(Af_h)(x)] = \\mathbb{E}[h(x)] - \\mathbb{E}[h(Z)]$\n",
    "\n",
    "## 1.4 Stein Discrepancy\n",
    "\n",
    "With the supremum, we have Stein discrepancy, which is equivalent to a statistical distance depends on the choice of $h$.\n",
    "\n",
    "$\\sup_{f_h} \\mathbb{E}_q[(A_p f_h)(X)] = \\sup_{h} \\mathbb{E}_q[h(X)] - \\mathbb{E}_p[h(Z)]$\n",
    "\n",
    "Defining the statistical distance defines a class of $h$. The set of $h$ defines the class of $f_h$.\n",
    "Note that $A_p$ is dependent on $p$. In other words, we need to know $p$ in advance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kernelized Stein Discrepancy\n",
    "\n",
    "## 2.1 Multivariate Stein's Operator\n",
    "\n",
    "So far we have written Stein's method in univariate setting. Expanding it involves the a vector-valued test function $\\mathbf{f}(\\mathbf{x})$.\n",
    "$$\n",
    "A_{p}\\mathbf{f}(\\mathbf{x}) =  \\nabla_{\\mathbf{x}}  \\log p(\\mathbf{x}) \\mathbf{f}(\\mathbf{x})^\\top + \\nabla_{\\mathbf{x}}\\mathbf{f}(\\mathbf{x})\n",
    "$$\n",
    "Then $A_{p}f$ is now a matrix-valued function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Kernelized Stein Discrepancy\n",
    "\n",
    "**Lemma from (Ley & Swan, 2013)** $\\mathbb{E}_p [A_q \\mathbf{f}(\\mathbf{x})] = \\mathbb{E}_p[(\\nabla\\log q(\\mathbf{x}) - \\nabla\\log p(\\mathbf{x})) \\mathbf{f(\\mathbf{x})}^\\top]$\n",
    "\n",
    "Stein discrepancy is the difference between two score functions weighted by $\\mathbf{x}$.\n",
    "To obtain a sclaer, the trace is taken. (the dimension of $f$ needs to be the same as $\\mathbf{x}$)\n",
    "\n",
    "$\\mathbb{E}_p [tr( A_q \\mathbf{f}(\\mathbf{x}))] = \\mathbb{E}_p[(\\nabla\\log q(\\mathbf{x}) - \\nabla\\log p(\\mathbf{x}))^\\top \\mathbf{f(\\mathbf{x})}]$\n",
    "\n",
    "We confine the set of $f$ to be a unit ball in an RKHS defined by a positive definite kernel function $k(\\cdot, \\cdot)$.\n",
    "\n",
    "KSD is obtained by squaring it.\n",
    "\n",
    "$S(p, q) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{x}' \\sim p}[(\\mathbf{s}_q (\\mathbf{x}) - \\mathbf{s}_p (\\mathbf{x}))^\\top f(\\mathbf{x}) f(\\mathbf{x}')^\\top (\\mathbf{s}_q (\\mathbf{x}') - \\mathbf{s}_p (\\mathbf{x}'))]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
